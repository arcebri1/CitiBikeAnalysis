{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/87.0.4280.88/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\loren\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.88]\n"
     ]
    }
   ],
   "source": [
    "#Open web browser/Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open URL of page to be scraped\n",
    "url = 'https://s3.amazonaws.com/tripdata/index.html'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# zipp=soup.find_all('td')\n",
    "# # link = zipp['a']\n",
    "# # href=zipp['href']\n",
    "\n",
    "# # print(f'{zipp}')\n",
    "# # print(f'{link}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = soup.find_all('td')\n",
    "\n",
    "# #Create a list to append the title and image\n",
    "# zip_files=[]\n",
    "\n",
    "# # Iterate through each product\n",
    "# for data in datasets:\n",
    "#     # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "#     link = data.find('a')\n",
    "#     href = link['href']\n",
    "# #     title = link.find('h3').text\n",
    "#     browser.visit(href)\n",
    "    \n",
    "#     # Create BeautifulSoup object; parse with 'html.parser'\n",
    "# #     html = browser.html\n",
    "# #     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     #Scrape to find the image url\n",
    "# #     down=soup.find('div', class_='downloads')\n",
    "# #     image_url=down.find('a')['href']\n",
    "    \n",
    "#     #Append the results to the list\n",
    "#     zip_files.append({\"zipfile\": href})\n",
    "    \n",
    "#     #Print the results\n",
    "#     print('-----------')\n",
    "#     print(href)\n",
    "# #     print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years_state_to_remove = ['index', 'JC', '2013', '2014', '2015', '2016', '2017']\n",
    "\n",
    "# links = [link[\"href\"] for link in soup.find_all('a') if not any(s in link[\"href\"] for s in years_state_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/tripdata/201801-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201802-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201803-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201804-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201805-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201806-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201807-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201808-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201809-citibike-tripdata.csv.zip\n",
      "Downloading https://s3.amazonaws.com/tripdata/201810-citibike-tripdata.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# # Download and unzip the zip files in the links\n",
    "# for link in links:\n",
    "#     print(f\"Downloading {link}\")\n",
    "#     r = requests.get(link)\n",
    "#     name = link.rsplit(\"/\", 1)[-1]\n",
    "#     with open(f\"{name}\", 'wb') as f:\n",
    "#         f.write(r.content)\n",
    "\n",
    "#     with zipfile.ZipFile(f\"{name}\", 'r') as z:\n",
    "#         z.extractall(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
